{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18dbd082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "with open(\"./save/dataset.json\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f1f61eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from scipy import sparse \n",
    "\n",
    "vec = joblib.load('./save/tfidf/vectorizer.joblib')\n",
    "X = sparse.load_npz('./save/tfidf/X.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a30b1682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 2650092)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8f67d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(\n",
    "    n_components=100,\n",
    "    n_iter=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "Xk = svd.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "804254b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 100\n"
     ]
    }
   ],
   "source": [
    "N, k = Xk.shape \n",
    "print(N, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dabd6d1",
   "metadata": {},
   "source": [
    "There are two options for navigable small world searches: greedy and beam. \n",
    "\n",
    "Greedy means pick a random node, move to the neighboring node with closest distance to query, terminate when current node is better than neighbors.\n",
    "\n",
    "Beam means pick a random node, push all `M` neighboring nodes to a `max_len=ef_query` heap, and explore the next best node. Terminate when the worst of the `k` nearest to query is better than the best of the `ef_query` within heap.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "358c4e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hnswlib\n",
    "import numpy as np \n",
    "\n",
    "index = hnswlib.Index(space='cosine', dim=k)\n",
    "index.init_index(\n",
    "    max_elements=N,\n",
    "    ef_construction=200, # when adding a node, size of minheap to determine best neighbors so far \n",
    "    M=32 # connections each document has in a navigable small world\n",
    ")\n",
    "\n",
    "ids = np.arange(N)\n",
    "index.add_items(Xk, ids)\n",
    "\n",
    "index.set_ef(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c3c9c684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_hnsw_svd(doc_ind: int, k: int = 20):\n",
    "    q = Xk[doc_ind]\n",
    "    labels, distances = index.knn_query(q, k=k+1) \n",
    "    labels, distances = labels[0], distances[0]\n",
    "    \n",
    "    print(f\"Query: {data[doc_ind]['title']}\\n\")\n",
    "    for idx, distance in zip(labels[1:], distances[1:]):\n",
    "        print(f\"{data[idx]['title']} {distance}\")\n",
    "    return [(distance, data[idx]['title']) for idx, distance in zip(labels[1:], distances[1:])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20322ae6",
   "metadata": {},
   "source": [
    "Cosine similarity is low between wikipedia articles because `vocab_size=2650092`. This is probably too large and so relationships are weak. Furthermore, common words probably occur too often and so the inverse document frequency weight is making values way too low.\n",
    "\n",
    "Anyway, this repository is meant more so to understand how to do nearest neighbor and not worry about whether the search actually means something. I think the search is good enough for a proof of concept. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "49a5e736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: River HomeLink\n",
      "\n",
      "Joseph Burr Tyrrell Elementary School 0.0537259578704834\n",
      "Tulsa School of Arts and Sciences 0.07285439968109131\n",
      "Juja 0.07441812753677368\n",
      "Loreto High School, Limuru 0.08009469509124756\n",
      "Rocky Hill High School 0.08073699474334717\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.053725958, 'Joseph Burr Tyrrell Elementary School'),\n",
       " (0.0728544, 'Tulsa School of Arts and Sciences'),\n",
       " (0.07441813, 'Juja'),\n",
       " (0.080094695, 'Loreto High School, Limuru'),\n",
       " (0.080736995, 'Rocky Hill High School')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_hnsw_svd(51231, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
