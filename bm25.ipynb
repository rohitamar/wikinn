{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e87e381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "with open(\"./save/dataset.json\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c51cf52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata \n",
    "\n",
    "def preprocess(s: str):\n",
    "    # don't know what this does ngl\n",
    "    s = unicodedata.normalize(\"NFC\", s).lower()\n",
    "    # remove hyphens\n",
    "    s = re.sub(r\"[-‐-‒–—]+\", \" \", s)\n",
    "    # accent folding\n",
    "    s = unicodedata.normalize(\"NFKD\", s).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    # whitespace for single space \n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = re.findall(r\"(?u)\\b[^\\W\\d_]+(?:'[^\\W\\d_]+)?\\b\", text)\n",
    "    return [t.lower() for t in tokens]\n",
    "\n",
    "docs = []\n",
    "for d in data:\n",
    "    doc = preprocess(d['title']) + \" \" + preprocess(d['text'])\n",
    "    docs.append(tokenize(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89203be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from heapq import nlargest\n",
    "import math \n",
    "\n",
    "class BM25:\n",
    "    def __init__(self, docs, k1=1.2, b=0.75):\n",
    "        self.k1, self.b = k1, b\n",
    "        self.N = len(docs)\n",
    "        self.doc_len = [0]*self.N\n",
    "        self.avgdl = 0.0\n",
    "        self.df = defaultdict(int)          \n",
    "        self.postings = defaultdict(list)   \n",
    "       \n",
    "        for i, tokens in enumerate(docs):\n",
    "            self.doc_len[i] = len(tokens)\n",
    "            self.avgdl += len(tokens)\n",
    "            tf = Counter(tokens)\n",
    "            for term, f in tf.items():\n",
    "                self.postings[term].append((i, f))\n",
    "                self.df[term] += 1\n",
    "        self.avgdl /= max(1, self.N)\n",
    "       \n",
    "        self.idf = {\n",
    "            t: math.log((self.N - df + 0.5) / (df + 0.5) + 1.0)\n",
    "            for t, df in self.df.items()\n",
    "        }\n",
    "    \n",
    "    def score(self, query, top_k=10):\n",
    "        q_terms = tokenize(query)\n",
    "        scores = defaultdict(float)\n",
    "        \n",
    "        for t in set(q_terms):\n",
    "            if t not in self.postings: \n",
    "                continue\n",
    "\n",
    "            idf = self.idf.get(t, 0.0)\n",
    "            for doc_id, f in self.postings[t]:\n",
    "                dl = self.doc_len[doc_id]\n",
    "                denom = f + self.k1 * (1.0 - self.b + self.b * dl / self.avgdl)\n",
    "                scores[doc_id] += idf * (f * (self.k1 + 1.0)) / denom\n",
    "        \n",
    "        return nlargest(top_k, scores.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75fd0521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robert C. Merton\n",
      "Riccardo Rebonato\n",
      "Risk-neutral measure\n",
      "Jim Gatheral\n",
      "Index of international trade articles\n",
      "John Y. Campbell\n",
      "Söhnke M. Bartram\n",
      "The Oxford Companion to Music\n",
      "Gregory D. Scholes\n",
      "Discounted cash flow\n"
     ]
    }
   ],
   "source": [
    "bm25 = BM25(docs, k1=1.2, b=0.75)\n",
    "\n",
    "nn_docs = bm25.score(\"quantitative finance option pricing black scholes\", top_k=10)\n",
    "\n",
    "for idx, _ in nn_docs:\n",
    "    print(data[idx]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77441aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_bm25(idx, top_k=10, prune_size=200):\n",
    "    tokens = docs[idx]\n",
    "    tf = Counter(tokens)\n",
    "    \n",
    "    # pick `prune_size` largest tf-idf to prune search (and ignore spurious/outliers)\n",
    "    weighted = [(t, tf[t] * bm25.idf.get(t, 0.0)) for t in tf]\n",
    "    weighted.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    q = \" \".join(t for t, _ in weighted[:prune_size])\n",
    "    res = bm25.score(q, top_k=top_k+1)\n",
    "    \n",
    "    print(f\"Query: {data[res[0][0]]['title']}\", end = \"\\n\\n\")\n",
    "    for idx, _ in res[1:]:\n",
    "        print(data[idx]['title']) \n",
    "    \n",
    "    return [(d,s) for d,s in res if d != idx][:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4b4ec51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Four Pillars of Destiny\n",
      "\n",
      "Records of Kangxi's Travel Incognito\n",
      "Solar term\n",
      "List of Chinese actors\n",
      "Huang Tsung-hsing\n",
      "The First Half of My Life\n",
      "List of Chinese philosophers\n",
      "Tseng Chang\n",
      "1911 (film)\n",
      "Eastern Wu family trees\n",
      "Liao Hua\n"
     ]
    }
   ],
   "source": [
    "_ = search_bm25(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
